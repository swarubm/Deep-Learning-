{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swarubm/Deep-Learning-/blob/main/DL01.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Design and implement a neural based network for generating word embedding (for words in a document corpus.)"
      ],
      "metadata": {
        "id": "cM6E_AWlXOlw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import Counter\n",
        "import random\n",
        "import numpy as np\n",
        "# Sample corpus (list of sentences; replace with your document corpus)\n",
        "corpus = [\n",
        "\"the quick brown fox jumps over the lazy dog\",\n",
        "\"word embeddings capture semantic meanings in vectors\",\n",
        "\"neural networks are powerful for natural language processing\",\n",
        "\"fork is built by xai to answer questions\"\n",
        "]\n",
        "# Preprocessing: Tokenize and build vocabulary\n",
        "tokens = [word for sentence in corpus for word in sentence.split()]\n",
        "vocab = list(set(tokens)) # Unique words\n",
        "vocab_size = len(vocab)\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "# Generate training data (skip-gram pairs)\n",
        "def generate_pairs(tokens, window_size=2):\n",
        "    pairs = []\n",
        "    for i, target in enumerate(tokens):\n",
        "        context = tokens[max(0, i - window_size):i] + tokens[i + 1:i + window_size + 1]\n",
        "        for ctx in context:\n",
        "            pairs.append((word_to_idx[target], word_to_idx[ctx]))\n",
        "    return pairs\n",
        "data = generate_pairs(tokens)\n",
        "# Negative sampling: Build frequency-based sampler for negatives\n",
        "word_freq = Counter(tokens)\n",
        "word_freq = {word: freq ** 0.75 for word, freq in word_freq.items()} # Unigram^0.75\n",
        "total_freq = sum(word_freq.values())\n",
        "neg_sampling_probs = {word: freq / total_freq for word, freq in word_freq.items()}\n",
        "def get_negatives(target, num_neg=5):\n",
        "    negatives = []\n",
        "    while len(negatives) < num_neg:\n",
        "        neg = random.choices(list(neg_sampling_probs.keys()),\n",
        "                            weights=list(neg_sampling_probs.values()))[0]\n",
        "        if neg != target:\n",
        "            negatives.append(word_to_idx[neg])\n",
        "    return negatives\n",
        "\n",
        "# Model Definition\n",
        "class SkipGramModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_dim):\n",
        "        super(SkipGramModel, self).__init__()\n",
        "        self.embeddings = nn.Embedding(vocab_size, embed_dim) # Target embeddings\n",
        "        self.context_embeddings = nn.Embedding(vocab_size, embed_dim) # Context embeddings\n",
        "    def forward(self, target, context):\n",
        "        target_emb = self.embeddings(target)\n",
        "        context_emb = self.context_embeddings(context)\n",
        "        return target_emb, context_emb\n",
        "# Training\n",
        "embed_dim = 100\n",
        "model = SkipGramModel(vocab_size, embed_dim)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCEWithLogitsLoss() # For negative sampling\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    total_loss = 0\n",
        "    for target_idx, context_idx in data:\n",
        "        # Positive sample\n",
        "        target = torch.tensor([target_idx], dtype=torch.long)\n",
        "        context_pos = torch.tensor([context_idx], dtype=torch.long)\n",
        "        target_emb, context_pos_emb = model(target, context_pos)\n",
        "        pos_score = torch.sum(target_emb * context_pos_emb, dim=1)\n",
        "        pos_label = torch.ones_like(pos_score)\n",
        "        # Negative samples\n",
        "        negs = get_negatives(idx_to_word[target_idx])\n",
        "        context_neg = torch.tensor(negs, dtype=torch.long)\n",
        "        _, context_neg_emb = model(target, context_neg) # Reuse target_emb\n",
        "        neg_score = torch.matmul(target_emb, context_neg_emb.t()).squeeze(0)\n",
        "        neg_label = torch.zeros_like(neg_score)\n",
        "        # Combined loss\n",
        "        scores = torch.cat([pos_score, neg_score])\n",
        "        labels = torch.cat([pos_label, neg_label])\n",
        "        loss = loss_fn(scores, labels)\n",
        "        total_loss += loss.item()\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "    print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(data):.4f}\")\n",
        "# Extract and print sample embeddings\n",
        "embeddings = model.embeddings.weight.data.numpy()\n",
        "print(\"\\nSample Word Embeddings:\")\n",
        "\n",
        "for word in [\"the\", \"word\", \"neural\", \"fork\"]:\n",
        "    if word in word_to_idx:\n",
        "        idx = word_to_idx[word]\n",
        "        print(f\"{word}: {embeddings[idx][:5]}... (first 5 dims)\")\n",
        "    else:\n",
        "        print(f\"{word}: Not in vocabulary\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTWkvl23XAKw",
        "outputId": "5a9d8bf7-b2e5-489f-9635-f7f734b72813"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.6817\n",
            "Epoch 2, Loss: 3.6933\n",
            "Epoch 3, Loss: 3.7358\n",
            "Epoch 4, Loss: 3.7836\n",
            "Epoch 5, Loss: 3.8697\n",
            "Epoch 6, Loss: 3.4025\n",
            "Epoch 7, Loss: 3.6687\n",
            "Epoch 8, Loss: 3.2660\n",
            "Epoch 9, Loss: 3.5055\n",
            "Epoch 10, Loss: 3.4188\n",
            "\n",
            "Sample Word Embeddings:\n",
            "the: [ 0.9357819   1.2592202   0.27914768  0.31786415 -1.3514616 ]... (first 5 dims)\n",
            "word: [-8.3842629e-01  3.7817198e-01 -1.1667944e+00  1.2757850e-03\n",
            " -1.2861936e+00]... (first 5 dims)\n",
            "neural: [ 0.62032795 -0.88399357  0.4986501  -0.14483026  0.8323627 ]... (first 5 dims)\n",
            "fork: [ 0.25814137  1.9439371  -1.0232241   1.1402344  -0.25352624]... (first 5 dims)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}